@video{aicoffeebreakwithletitiaUltimateIntroGraph2020,
  title = {The Ultimate Intro to {{Graph Neural Networks}}. {{Maybe}}.},
  editor = {{AI Coffee Break with Letitia}},
  date = {2020-08-13},
  url = {https://www.youtube.com/watch?v=me3UsMm9QEs},
  urldate = {2022-12-06},
  editortype = {director}
}

@misc{huangPrivacyAwareCausalStructure2022,
  title = {Towards {{Privacy-Aware Causal Structure Learning}} in {{Federated Setting}}},
  author = {Huang, Jianli and Yu, Kui and Guo, Xianjie and Cao, Fuyuan and Liang, Jiye},
  date = {2022-11-13},
  number = {arXiv:2211.06919},
  eprint = {2211.06919},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.06919},
  url = {http://arxiv.org/abs/2211.06919},
  urldate = {2023-02-06},
  abstract = {Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attached much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel Federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strategy for learning consistent separation sets for federated edge orientation. The extensive experiments validate that FedPC is effective for causal structure learning in federated learning setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/tongyuan/Zotero/storage/C5WM88FN/Huang et al_2022_Towards Privacy-Aware Causal Structure Learning in Federated Setting.pdf;/Users/tongyuan/Zotero/storage/V869FDWF/2211.html}
}

@inproceedings{ngFederatedBayesianNetwork2022,
  title = {Towards {{Federated Bayesian Network Structure Learning}} with {{Continuous Optimization}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ng, Ignavier and Zhang, Kun},
  date = {2022-05-03},
  pages = {8095--8111},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v151/ng22a.html},
  urldate = {2023-02-06},
  abstract = {Traditionally, Bayesian network structure learning is often carried out at a central site, in which all data is gathered. However, in practice, data may be distributed across different parties (e.g., companies, devices) who intend to collectively learn a Bayesian network, but are not willing to disclose information related to their data owing to privacy or security concerns. In this work, we present a federated learning approach to estimate the structure of Bayesian network from data that is horizontally partitioned across different parties. We develop a distributed structure learning method based on continuous optimization, using the alternating direction method of multipliers (ADMM), such that only the model parameters have to be exchanged during the optimization process. We demonstrate the flexibility of our approach by adopting it for both linear and nonlinear cases. Experimental results on synthetic and real datasets show that it achieves an improved performance over the other methods, especially when there is a relatively large number of clients and each has a limited sample size.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/tongyuan/Zotero/storage/SWMLU5FA/Ng_Zhang_2022_Towards Federated Bayesian Network Structure Learning with Continuous.pdf}
}

@video{vgogateStructureLearningAlgorithms2020,
  title = {Structure {{Learning Algorithms}} for {{Bayesian Networks}}},
  editor = {{vgogate}},
  date = {2020-04-19},
  url = {https://www.youtube.com/watch?v=8N0HsrBY7WI},
  urldate = {2023-01-05},
  editortype = {director},
  keywords = {/unread}
}

@article{wenThunderGBMFastGBDTs,
  title = {{{ThunderGBM}}: {{Fast GBDTs}} and {{Random Forests}} on {{GPUs}}},
  author = {Wen, Zeyi and Liu, Hanfeng and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  pages = {5},
  abstract = {Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail. Documentation, examples, and more details about ThunderGBM are available at https://github.com/xtra-computing/thundergbm.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/tongyuan/Zotero/storage/F5W9MZ9I/Wen et al. - ThunderGBM Fast GBDTs and Random Forests on GPUs.pdf}
}

@article{wenThunderSVMFastSVM,
  title = {{{ThunderSVM}}: {{A Fast SVM Library}} on {{GPUs}} and {{CPUs}}},
  author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  pages = {5},
  abstract = {Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26\% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities—including classification (SVC), regression (SVR) and one-class SVMs—of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/tongyuan/Zotero/storage/U4HMBTEJ/Wen et al. - ThunderSVM A Fast SVM Library on GPUs and CPUs.pdf}
}
