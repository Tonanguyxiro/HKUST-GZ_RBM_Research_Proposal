@video{aicoffeebreakwithletitiaUltimateIntroGraph2020,
  title = {The Ultimate Intro to {{Graph Neural Networks}}. {{Maybe}}.},
  editor = {{AI Coffee Break with Letitia}},
  date = {2020-08-13},
  url = {https://www.youtube.com/watch?v=me3UsMm9QEs},
  urldate = {2022-12-06},
  editortype = {director}
}

@misc{huangPrivacyAwareCausalStructure2022,
  title = {Towards {{Privacy-Aware Causal Structure Learning}} in {{Federated Setting}}},
  author = {Huang, Jianli and Yu, Kui and Guo, Xianjie and Cao, Fuyuan and Liang, Jiye},
  date = {2022-11-13},
  number = {arXiv:2211.06919},
  eprint = {2211.06919},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.06919},
  url = {http://arxiv.org/abs/2211.06919},
  urldate = {2023-02-06},
  abstract = {Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attached much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel Federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strategy for learning consistent separation sets for federated edge orientation. The extensive experiments validate that FedPC is effective for causal structure learning in federated learning setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/tongyuan/Zotero/storage/C5WM88FN/Huang et al_2022_Towards Privacy-Aware Causal Structure Learning in Federated Setting.pdf;/Users/tongyuan/Zotero/storage/V869FDWF/2211.html}
}

@inproceedings{ngFederatedBayesianNetwork2022,
  title = {Towards {{Federated Bayesian Network Structure Learning}} with {{Continuous Optimization}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ng, Ignavier and Zhang, Kun},
  date = {2022-05-03},
  pages = {8095--8111},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v151/ng22a.html},
  urldate = {2023-02-06},
  abstract = {Traditionally, Bayesian network structure learning is often carried out at a central site, in which all data is gathered. However, in practice, data may be distributed across different parties (e.g., companies, devices) who intend to collectively learn a Bayesian network, but are not willing to disclose information related to their data owing to privacy or security concerns. In this work, we present a federated learning approach to estimate the structure of Bayesian network from data that is horizontally partitioned across different parties. We develop a distributed structure learning method based on continuous optimization, using the alternating direction method of multipliers (ADMM), such that only the model parameters have to be exchanged during the optimization process. We demonstrate the flexibility of our approach by adopting it for both linear and nonlinear cases. Experimental results on synthetic and real datasets show that it achieves an improved performance over the other methods, especially when there is a relatively large number of clients and each has a limited sample size.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/tongyuan/Zotero/storage/SWMLU5FA/Ng_Zhang_2022_Towards Federated Bayesian Network Structure Learning with Continuous.pdf}
}

@video{vgogateStructureLearningAlgorithms2020,
  title = {Structure {{Learning Algorithms}} for {{Bayesian Networks}}},
  editor = {{vgogate}},
  date = {2020-04-19},
  url = {https://www.youtube.com/watch?v=8N0HsrBY7WI},
  urldate = {2023-01-05},
  editortype = {director},
  keywords = {/unread}
}

@article{wenThunderGBMFastGBDTs,
  title = {{{ThunderGBM}}: {{Fast GBDTs}} and {{Random Forests}} on {{GPUs}}},
  author = {Wen, Zeyi and Liu, Hanfeng and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  pages = {5},
  abstract = {Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail. Documentation, examples, and more details about ThunderGBM are available at https://github.com/xtra-computing/thundergbm.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/tongyuan/Zotero/storage/F5W9MZ9I/Wen et al. - ThunderGBM Fast GBDTs and Random Forests on GPUs.pdf}
}

@article{wenThunderSVMFastSVM,
  title = {{{ThunderSVM}}: {{A Fast SVM Library}} on {{GPUs}} and {{CPUs}}},
  author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
  pages = {5},
  abstract = {Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26\% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities‚Äîincluding classification (SVC), regression (SVR) and one-class SVMs‚Äîof LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/tongyuan/Zotero/storage/U4HMBTEJ/Wen et al. - ThunderSVM A Fast SVM Library on GPUs and CPUs.pdf}
}

@inproceedings{jiangFastParallelBayesian2022a,
  title = {Fast {{Parallel Bayesian Network Structure Learning}}},
  booktitle = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Jiang, Jiantong and Wen, Zeyi and Mian, Ajmal},
  year = {2022},
  month = may,
  eprint = {2212.04259},
  primaryclass = {cs},
  pages = {617--627},
  doi = {10.1109/IPDPS53621.2022.00066},
  urldate = {2022-12-19},
  abstract = {Bayesian networks (BNs) are a widely used graphical model in machine learning for representing knowledge with uncertainty. The mainstream BN structure learning methods require performing a large number of conditional independence (CI) tests. The learning process is very time-consuming, especially for high-dimensional problems, which hinders the adoption of BNs to more applications. Existing works attempt to accelerate the learning process with parallelism, but face issues including load unbalancing, costly atomic operations and dominant parallel overhead. In this paper, we propose a fast solution named Fast-BNS on multi-core CPUs to enhance the efficiency of the BN structure learning. Fast-BNS is powered by a series of efficiency optimizations including (i) designing a dynamic work pool to monitor the processing of edges and to better schedule the workloads among threads, (ii) grouping the CI tests of the edges with the same endpoints to reduce the number of unnecessary CI tests, (iii) using a cache-friendly data storage to improve the memory efficiency, and (iv) generating the conditioning sets on-the-fly to avoid extra memory consumption. A comprehensive experimental study shows that the sequential version of Fast-BNS is up to 50 times faster than its counterpart, and the parallel version of Fast-BNS achieves 4.8 to 24.5 times speedup over the state-of-the-art multi-threaded solution. Moreover, Fast-BNS has a good scalability to the network size as well as sample size. Fast-BNS source code is freely available at https://github.com/jjiantong/FastBN.},
  archiveprefix = {arxiv},
  keywords = {üëç,Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {/Users/tongyuan/Zotero/storage/PEDBV5G9/Jiang et al. - 2022 - Fast Parallel Bayesian Network Structure Learning.pdf;/Users/tongyuan/Zotero/storage/NYPA4UYD/2212.html}
}

